# Functions_Paths.R
# Shared paths & helper functions

##Packages
suppressPackageStartupMessages({
  library(sf)
  library(dplyr)
  library(purrr)
  library(nhdplusTools)
  library(hydroloom)
  library(dataRetrieval)
  library(tidyverse)
  library(arrow)
  library(slider)
  library(lubridate)
  library(zoo)
})

## Paths
### Assumes your working directory is set to the repo root
base_dir <- getwd()

data_dir          <- file.path(base_dir, "data")
shape_dir         <- file.path(data_dir, "shapes")
nhd_dir           <- file.path(data_dir, "nhdplus")
meteo_dir         <- file.path(data_dir, "meteo")
daily_drivers_dir <- file.path(data_dir, "daily_drivers")

### If directory doesn't exist it will create it otherwise move on
ensure_dir <- function(path) {
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE, showWarnings = FALSE)
  }
  invisible(path)
}

### AOI shapefile (Illinois River basin)
basin_shp <- file.path(shape_dir, "illinois_river_basin.shp")


### Ensure all expected directories exist
ensure_dir(data_dir)
ensure_dir(shape_dir)
ensure_dir(nhd_dir)
ensure_dir(meteo_dir)
ensure_dir(daily_drivers_dir)

### File Paths
catchment_path <- file.path(nhd_dir, "catchments.gpkg")
basins_path    <- file.path(shape_dir, "site_upstream_basins.gpkg")
flowlines_path <- file.path(nhd_dir, "flowlines.gpkg")
sites_path <- file.path(shape_dir, "sites_with_comid.shp")

### Load the merged climate drivers dataset from Feather
drivers_path  <- file.path(daily_drivers_dir, "drivers_daily_joined.feather")

### Merged climate drives + PET
daily_drivers_out <- file.path(daily_drivers_dir, "drivers_daily_joined_PET.feather")

### Discharge obs path
q_path <- file.path(daily_drivers_dir, "q_obs.parquet")

# Water temp metadata
meta_out <- file.path(daily_drivers_dir, "illinois_water_temp_metadata.csv")

# Temp with comid out
temp_comid_out <- file.path(daily_drivers_dir, "illinois_water_temp_comid.csv")

### Hydro and temp data out 
out_path_nodes_temp <- file.path(daily_drivers_dir, "hydro_temp_data.parquet")
### Edges and Flowlines Path
out_path_nodes <- file.path(daily_drivers_dir, "hydro_data.parquet")
out_path_edges <- file.path(daily_drivers_dir, "edge_list.parquet")

## Functions
### Geopackage Overwrite
write_gpkg_overwrite <- function(obj, path) {
  if (file.exists(path)) file.remove(path)
  st_write(obj, path, quiet = TRUE)
  invisible(path)
}

### Extract Basins from nhdplus
get_basin_safe <- purrr::possibly(
  function(cid) {
    b <- get_nldi_basin(list(featureSource = "comid", featureID = cid))
    if (is.null(b)) return(NULL)
    dplyr::mutate(b, COMID = cid)
  },
  otherwise = NULL
)

### Date cleaning for GridMET names
#### e.g., "Xpr_1984.01.01" -> "1984-01-01"

clean_date <- function(raw_name) {
  raw_name <- sub("^X", "", raw_name)           # remove leading X
  raw_name <- sub("^[A-Za-z]+_", "", raw_name)  # remove prefixes like pr_, tmmx_
  raw_name <- gsub("\\.", "-", raw_name)        # dots → hyphens
  raw_name <- trimws(raw_name)                  # remove spaces
  as.Date(raw_name, format = "%Y-%m-%d")
}

###Loops over years between start_date and end_date so a timeout / failure
#### in one year doesn't kill the whole multi-decade pull.
#### Requires: climateR, terra, exactextractr, sf

extract_variable <- function(varname,
                             polygons,
                             start_date,
                             end_date,
                             out_dir,
                             id_col = "comid") {
  message("Processing variable: ", varname)
  
  # Normalize dates
  start_date <- as.Date(start_date)
  end_date   <- as.Date(end_date)
  
  if (start_date > end_date) {
    stop("start_date must be <= end_date")
  }
  
  # Ensure output directory exists
  ensure_dir(out_dir)
  
  # List to accumulate per-year data
  all_years_list <- list()
  
  # Keep original polygons; we'll transform to raster CRS once
  polygons_orig <- polygons
  polygons_r    <- NULL
  rast_crs      <- NULL
  
  years <- seq(as.integer(format(start_date, "%Y")),
               as.integer(format(end_date,   "%Y")))
  
  for (yy in years) {
    # Chunk window restricted to this year and overall requested range
    chunk_start <- max(start_date, as.Date(sprintf("%d-01-01", yy)))
    chunk_end   <- min(end_date,   as.Date(sprintf("%d-12-31", yy)))
    
    if (chunk_start > chunk_end) next
    
    message("  Year ", yy, ": ", chunk_start, " to ", chunk_end)
    
    # Pull GridMET for this year
    r_list <- tryCatch(
      climateR::getGridMET(
        AOI       = polygons_orig,
        varname   = varname,
        startDate = as.character(chunk_start),
        endDate   = as.character(chunk_end)
      ),
      error = function(e) {
        message("   Skipping year ", yy, " due to error: ", e$message)
        return(NULL)
      }
    )
    
    if (is.null(r_list)) next
    
    # Convert to terra raster
    if (is.list(r_list)) {
      r <- terra::rast(r_list)
    } else {
      r <- r_list
    }
    
    # On first successful year, align polygons to raster CRS once
    if (is.null(rast_crs)) {
      rast_crs  <- terra::crs(r, proj = TRUE)
      polygons_r <- sf::st_transform(polygons_orig, rast_crs)
    }
    
    # Clean layer names into dates
    r_dates <- names(r)
    r_dates <- clean_date(r_dates)
    names(r) <- as.character(r_dates)
    
    # Extract daily means for each polygon in this year
    df_list <- lapply(seq_along(r_dates), function(i) {
      day_vals <- exactextractr::exact_extract(r[[i]], polygons_r, fun = "mean")
      tmp <- data.frame(
        id    = polygons_r[[id_col]],
        date  = r_dates[i],
        value = day_vals
      )
      names(tmp)[1] <- id_col
      tmp
    })
    
    year_df <- do.call(rbind, df_list)
    all_years_list[[length(all_years_list) + 1]] <- year_df
  }
  
  if (!length(all_years_list)) {
    warning("No data returned for variable ", varname,
            " between ", start_date, " and ", end_date)
    return(data.frame())
  }
  
  # Combine all years
  df_long <- do.call(rbind, all_years_list)
  colnames(df_long)[3] <- varname
  
  # Write per-variable CSV
  out_file <- file.path(out_dir, paste0(varname, "_daily.csv"))
  write.csv(df_long, out_file, row.names = FALSE)
  message("Saved ", varname, " to: ", out_file)
  
  df_long
}
### Calculate extraterrestrial radiation (Ra)
#### lat in degrees, DOY as integer (1–365/366)
compute_Ra <- function(lat, DOY) {
  Gsc   <- 0.0820                                  # solar constant (MJ m-2 min-1)
  dr    <- 1 + 0.033 * cos(2 * pi / 365 * DOY)     # inverse relative distance Earth-Sun
  delta <- 0.409 * sin(2 * pi / 365 * DOY - 1.39)  # solar declination
  phi   <- lat * pi / 180                          # latitude in radians
  omega <- acos(-tan(phi) * tan(delta))            # sunset hour angle
  
  Ra_MJ <- (24 * 60 / pi) * Gsc * dr *
    (omega * sin(phi) * sin(delta) + cos(phi) * cos(delta) * sin(omega))
  
  # Convert MJ m-2 day-1 to mm/day (FAO: 1 MJ/m2/day ≈ 0.408 mm/day)
  Ra_MJ * 0.408
}

# Uses global start_date / end_date defined in the calling script.
download_temp <- function(site_id) {
  tryCatch({
    message("Downloading: ", site_id, " from ", start_date, " to ", end_date)
    
    # Strip "USGS-" prefix because NWIS expects just the site number
    site_num <- sub("USGS-", "", site_id)
    
    df <- dataRetrieval::readNWISdv(
      siteNumbers = site_num,
      parameterCd = "00010",  # water temperature (°C)
      startDate   = as.character(start_date),
      endDate     = as.character(end_date)
    )
    
    if (nrow(df) > 0) {
      df <- dataRetrieval::renameNWISColumns(df)
      df$monitoring_location_id <- site_id
      return(df)
    } else {
      return(NULL)
    }
  }, error = function(e) {
    message("Error for site ", site_id, ": ", e$message)
    return(NULL)
  })
}
